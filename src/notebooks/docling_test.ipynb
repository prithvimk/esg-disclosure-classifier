{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca53bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "316c37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source = r\"sample.pdf\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "# print(result.document.export_to_markdown())  # output: \"## Docling Technical Report[...]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b853dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eeadd6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document structure breakdown:\n",
      "  picture: 1 elements\n",
      "  section_header: 4 elements\n",
      "  text: 15 elements\n",
      "  list_item: 14 elements\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "element_types = defaultdict(list)\n",
    "\n",
    "# Iterate through all document elements and group them by label\n",
    "for item, _ in result.document.iterate_items():\n",
    "    element_type = item.label\n",
    "    element_types[element_type].append(item)\n",
    "\n",
    "# Display the breakdown of document structure\n",
    "print(\"Document structure breakdown:\")\n",
    "for element_type, items in element_types.items():\n",
    "    print(f\"  {element_type}: {len(items)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28b6a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = result.document # DoclingDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2025ce32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dict = doc.export_to_dict()\n",
    "\n",
    "json_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8909354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized for large documents\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    max_num_pages=4,  # Limit processing to first 4 pages\n",
    "    page_range=[1, 3],  # Process specific page range\n",
    "    generate_page_images=False,  # Skip page images to save memory\n",
    "    do_table_structure=False,  # Skip table structure extraction\n",
    "    enable_parallel_processing=True  # Use multiple cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdc15ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridChunker: 6 chunks\n",
      "Chunk length: 831 characters\n",
      "Chunk content: TORONTO, July 15, 2025 /CNW/ I....isoenergy.ca/sustainability/.\n",
      "--------------------------------------------------\n",
      "Chunk length: 2241 characters\n",
      "Chunk content: - Environmental achievements i...sustainability policy in 2025.\n",
      "--------------------------------------------------\n",
      "Chunk length: 876 characters\n",
      "Chunk content: IsoEnergy (NYSE American: ISOU...@IsoEnergyLtd www.isoenergy.ca\n",
      "--------------------------------------------------\n",
      "Chunk length: 956 characters\n",
      "Chunk content: This press release contains \"f...ll or may occur in the future.\n",
      "--------------------------------------------------\n",
      "Chunk length: 1443 characters\n",
      "Chunk content: Forw ard-looking statements ar...n forw ard-looking statements.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "# Process with HybridChunker (token-aware)\n",
    "hybrid_chunker = HybridChunker(max_tokens=512, overlap_tokens=50)\n",
    "hybrid_chunks = list(hybrid_chunker.chunk(doc))\n",
    "\n",
    "print(f\"HybridChunker: {len(hybrid_chunks)} chunks\")\n",
    "\n",
    "def print_chunk(chunk):\n",
    "    print(f\"Chunk length: {len(chunk.text)} characters\")\n",
    "    if len(chunk.text) > 30:\n",
    "        print(f\"Chunk content: {chunk.text[:30]}...{chunk.text[-30:]}\")\n",
    "    else:\n",
    "        print(f\"Chunk content: {chunk.text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print the first 3 chunks\n",
    "for chunk in hybrid_chunks[:5]:\n",
    "    print_chunk(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9b859",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- Docling gives great results \n",
    "- the advanced pipeline options should be explored for larger docs\n",
    "- hybrid chunking seems to be the right approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ca317",
   "metadata": {},
   "source": [
    "### NLP based approach for data extraction\n",
    "- faster, no-cost soln as compared to llm-based extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54597c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c5b15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- NLP SETUP ----------\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# ESG keywords & patterns\n",
    "ESG_KEYWORDS = [\n",
    "    \"emissions\", \"carbon\", \"CO2\", \"renewable\", \"sustainable\", \"ESG\",\n",
    "    \"waste\", \"diversity\", \"gender\", \"compliance\", \"regulation\", \"ISO\",\n",
    "    \"investment\", \"energy\", \"greenhouse\", \"climate\"\n",
    "]\n",
    "\n",
    "PATTERNS = [\n",
    "    re.compile(r\"reduced\\s+\\w+\\s+emissions\\s+by\\s+\\d+%.*\", re.IGNORECASE),\n",
    "    re.compile(r\"complies\\s+with\\s+.+\", re.IGNORECASE),\n",
    "    re.compile(r\"invest(ed|ment)\\s+in\\s+.+\", re.IGNORECASE),\n",
    "    re.compile(r\"achiev(ed|ement)\\s+of\\s+\\d+%.*\", re.IGNORECASE)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "791832c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_extract(text):\n",
    "    \"\"\"Apply keyword filtering and regex to extract candidate ESG sentences.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    candidates = []\n",
    "    uncertain = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        if any(k in sent_text.lower() for k in ESG_KEYWORDS):\n",
    "            if any(p.search(sent_text) for p in PATTERNS):\n",
    "                # Direct extraction from rule\n",
    "                candidates.append({\n",
    "                    \"subject\": \"Unknown\",  # Will refine later\n",
    "                    \"predicate\": \"esg_fact\",\n",
    "                    \"object\": sent_text\n",
    "                })\n",
    "            else:\n",
    "                # Keyword found but no exact match â†’ send to LLM\n",
    "                uncertain.append(sent_text)\n",
    "\n",
    "    return candidates, uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddd87049",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = hybrid_chunks[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd573470",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, uncertain = rule_based_extract(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "624edf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Environmental achievements include reducing impact and advancing operational readiness.\\n- Achieved zero significant environmental incidents across all projects in 2024.\\n- Initiated baseline environmental studies at Larocque East to guide future permitting and project design with a view to minimizing ecological impacts.\\n- Reclaimed all active exploration sites, and enhanced waste management practices with the goal of reducing material waste.\\n- Improved water efficiency and reduced emissions at Tony M Mine through targeted infrastructure upgrades.\\n- Partnered with communities, establishing investments in people.\\n- Maintained strong Indigenous representation in the workforce with 63% at Matoush and 36% at Larocque East.\\n- Supported community well-being initiatives, including the Northlands College Scholarship Foundation and JZ Memorial Fund.\\n- Implemented Company-wide policies on Health and Safety, Respectful Workplace, and Diversity.\\n- Standardized incident investigation reporting across all exploration projects.\\n- Strengthened governance with new policies for sustainable growth.\\n- Adopted new oversight measures, including Corporate Governance Guidelines and Majority Voting Policy.\\n- Achieved 100% Code of Ethics compliance, reinforcing a culture of integrity.\\n- Continued strong Board oversight on risk management, cybersecurity, and sustainability.\\nPhilip Williams, CEO and Director of IsoEnergy, commented, \"This inaugural Report underscores our commitment to responsible resource development and highlights the concrete steps we have taken to reduce our environmental footprint, invest in local communities and businesses, and strengthen partnerships with Indigenous Nations. As we continue advancing our projects, sustainability remains central to how we plan to operate and grow. Looking ahead to 2025, our focus is on formalizing IsoEnergy\\'s sustainability approach by developing a robust ESG framework to guide, manage, and track performance on material ESG issues.\"\\nThe Report sets a strong foundation for IsoEnergy\\'s sustainability journey and outlines the Company\\'s plan to formalize its ESG strategy, including plans to conduct a materiality assessment and develop a Company-wide sustainability policy in 2025.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2e381fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Reclaimed all active exploration sites, and enhanced waste management practices with the goal of reducing material waste.\\n- Improved water efficiency and reduced emissions at Tony M Mine through targeted infrastructure upgrades.\\n- Partnered with communities, establishing investments in people.\\n- Maintained strong Indigenous representation in the workforce with 63% at Matoush and 36% at Larocque East.\\n- Supported community well-being initiatives, including the Northlands College Scholarship Foundation and JZ Memorial Fund.\\n- Implemented Company-wide policies on Health and Safety, Respectful Workplace, and Diversity.\\n- Standardized incident investigation reporting across all exploration projects.\\n- Strengthened governance with new policies for sustainable growth.\\n- Adopted new oversight measures, including Corporate Governance Guidelines and Majority Voting Policy.',\n",
       " '- Achieved 100% Code of Ethics compliance, reinforcing a culture of integrity.\\n- Continued strong Board oversight on risk management, cybersecurity, and sustainability.',\n",
       " 'Philip Williams, CEO and Director of IsoEnergy, commented, \"This inaugural Report underscores our commitment to responsible resource development and highlights the concrete steps we have taken to reduce our environmental footprint, invest in local communities and businesses, and strengthen partnerships with Indigenous Nations.',\n",
       " \"Looking ahead to 2025, our focus is on formalizing IsoEnergy's sustainability approach by developing a robust ESG framework to guide, manage, and track performance on material ESG issues.\",\n",
       " \"The Report sets a strong foundation for IsoEnergy's sustainability journey and outlines the Company's plan to formalize its ESG strategy, including plans to conduct a materiality assessment and develop a Company-wide sustainability policy in 2025.\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f9680",
   "metadata": {},
   "source": [
    "spacy has failed miserably..\n",
    "\n",
    "trying out specialized ESGBert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62e00d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'RobertaForSequenceClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DiffLlamaForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GemmaForTokenClassification', 'Gemma2ForTokenClassification', 'GlmForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'HeliumForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LlamaForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MistralForTokenClassification', 'MixtralForTokenClassification', 'MobileBertForTokenClassification', 'ModernBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'MT5ForTokenClassification', 'NemotronForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'PersimmonForTokenClassification', 'PhiForTokenClassification', 'Phi3ForTokenClassification', 'QDQBertForTokenClassification', 'Qwen2ForTokenClassification', 'Qwen2MoeForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'StableLmForTokenClassification', 'Starcoder2ForTokenClassification', 'T5ForTokenClassification', 'UMT5ForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "### MAKE SURE TO INSTALL THIS LIB: !pip install transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline # for using the models\n",
    "\n",
    "### Load the models (takes ca. 1 min)\n",
    "# Environmental model.\n",
    "name = \"ESGBERT/EnvironmentalBERT-environmental\" # path to download from HuggingFace\n",
    "# In simple words, the tokenizer prepares the text for the model and the model classifies the text-\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "# The pipeline combines tokenizer and model to one process.\n",
    "pipe_env = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Also load the social and governance model.\n",
    "# Social model.\n",
    "name = \"ESGBERT/SocialBERT-social\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "pipe_soc = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Governance model.\n",
    "name = \"ESGBERT/GovernanceBERT-governance\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "pipe_gov = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
