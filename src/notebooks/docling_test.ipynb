{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca53bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316c37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 18:37:22.536000 14968 Lib\\site-packages\\torch\\utils\\cpp_extension.py:466] Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "W0815 18:37:22.537000 14968 Lib\\site-packages\\torch\\utils\\cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0815 18:37:22.537000 14968 Lib\\site-packages\\torch\\utils\\cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
      "Could not load the custom kernel for multi-scale deformable attention: Command '['where', 'cl']' returned non-zero exit status 1.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source = r\"sample.pdf\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "# print(result.document.export_to_markdown())  # output: \"## Docling Technical Report[...]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b853dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeadd6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document structure breakdown:\n",
      "  picture: 1 elements\n",
      "  section_header: 4 elements\n",
      "  text: 15 elements\n",
      "  list_item: 14 elements\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "element_types = defaultdict(list)\n",
    "\n",
    "# Iterate through all document elements and group them by label\n",
    "for item, _ in result.document.iterate_items():\n",
    "    element_type = item.label\n",
    "    element_types[element_type].append(item)\n",
    "\n",
    "# Display the breakdown of document structure\n",
    "print(\"Document structure breakdown:\")\n",
    "for element_type, items in element_types.items():\n",
    "    print(f\"  {element_type}: {len(items)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b6a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = result.document # DoclingDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2025ce32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dict = doc.export_to_dict()\n",
    "\n",
    "json_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8909354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized for large documents\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    max_num_pages=4,  # Limit processing to first 4 pages\n",
    "    page_range=[1, 3],  # Process specific page range\n",
    "    generate_page_images=False,  # Skip page images to save memory\n",
    "    do_table_structure=False,  # Skip table structure extraction\n",
    "    enable_parallel_processing=True  # Use multiple cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc15ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridChunker: 6 chunks\n",
      "Chunk length: 831 characters\n",
      "Chunk content: TORONTO, July 15, 2025 /CNW/ I....isoenergy.ca/sustainability/.\n",
      "--------------------------------------------------\n",
      "Chunk length: 2241 characters\n",
      "Chunk content: - Environmental achievements i...sustainability policy in 2025.\n",
      "--------------------------------------------------\n",
      "Chunk length: 876 characters\n",
      "Chunk content: IsoEnergy (NYSE American: ISOU...@IsoEnergyLtd www.isoenergy.ca\n",
      "--------------------------------------------------\n",
      "Chunk length: 956 characters\n",
      "Chunk content: This press release contains \"f...ll or may occur in the future.\n",
      "--------------------------------------------------\n",
      "Chunk length: 1443 characters\n",
      "Chunk content: Forw ard-looking statements ar...n forw ard-looking statements.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "# Process with HybridChunker (token-aware)\n",
    "hybrid_chunker = HybridChunker(max_tokens=512, overlap_tokens=50)\n",
    "hybrid_chunks = list(hybrid_chunker.chunk(doc))\n",
    "\n",
    "print(f\"HybridChunker: {len(hybrid_chunks)} chunks\")\n",
    "\n",
    "def print_chunk(chunk):\n",
    "    print(f\"Chunk length: {len(chunk.text)} characters\")\n",
    "    if len(chunk.text) > 30:\n",
    "        print(f\"Chunk content: {chunk.text[:30]}...{chunk.text[-30:]}\")\n",
    "    else:\n",
    "        print(f\"Chunk content: {chunk.text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print the first 3 chunks\n",
    "for chunk in hybrid_chunks[:5]:\n",
    "    print_chunk(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9b859",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- Docling gives great results \n",
    "- the advanced pipeline options should be explored for larger docs\n",
    "- hybrid chunking seems to be the right approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ca317",
   "metadata": {},
   "source": [
    "### NLP based approach for data extraction\n",
    "- faster, no-cost soln as compared to llm-based extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54597c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c5b15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- NLP SETUP ----------\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# ESG keywords & patterns\n",
    "ESG_KEYWORDS = [\n",
    "    \"emissions\", \"carbon\", \"CO2\", \"renewable\", \"sustainable\", \"ESG\",\n",
    "    \"waste\", \"diversity\", \"gender\", \"compliance\", \"regulation\", \"ISO\",\n",
    "    \"investment\", \"energy\", \"greenhouse\", \"climate\"\n",
    "]\n",
    "\n",
    "PATTERNS = [\n",
    "    re.compile(r\"reduced\\s+\\w+\\s+emissions\\s+by\\s+\\d+%.*\", re.IGNORECASE),\n",
    "    re.compile(r\"complies\\s+with\\s+.+\", re.IGNORECASE),\n",
    "    re.compile(r\"invest(ed|ment)\\s+in\\s+.+\", re.IGNORECASE),\n",
    "    re.compile(r\"achiev(ed|ement)\\s+of\\s+\\d+%.*\", re.IGNORECASE)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "791832c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_extract(text):\n",
    "    \"\"\"Apply keyword filtering and regex to extract candidate ESG sentences.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    candidates = []\n",
    "    uncertain = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        if any(k in sent_text.lower() for k in ESG_KEYWORDS):\n",
    "            if any(p.search(sent_text) for p in PATTERNS):\n",
    "                # Direct extraction from rule\n",
    "                candidates.append({\n",
    "                    \"subject\": \"Unknown\",  # Will refine later\n",
    "                    \"predicate\": \"esg_fact\",\n",
    "                    \"object\": sent_text\n",
    "                })\n",
    "            else:\n",
    "                # Keyword found but no exact match â†’ send to LLM\n",
    "                uncertain.append(sent_text)\n",
    "\n",
    "    return candidates, uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd87049",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = hybrid_chunks[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd573470",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, uncertain = rule_based_extract(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "624edf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Environmental achievements include reducing impact and advancing operational readiness.\\n- Achieved zero significant environmental incidents across all projects in 2024.\\n- Initiated baseline environmental studies at Larocque East to guide future permitting and project design with a view to minimizing ecological impacts.\\n- Reclaimed all active exploration sites, and enhanced waste management practices with the goal of reducing material waste.\\n- Improved water efficiency and reduced emissions at Tony M Mine through targeted infrastructure upgrades.\\n- Partnered with communities, establishing investments in people.\\n- Maintained strong Indigenous representation in the workforce with 63% at Matoush and 36% at Larocque East.\\n- Supported community well-being initiatives, including the Northlands College Scholarship Foundation and JZ Memorial Fund.\\n- Implemented Company-wide policies on Health and Safety, Respectful Workplace, and Diversity.\\n- Standardized incident investigation reporting across all exploration projects.\\n- Strengthened governance with new policies for sustainable growth.\\n- Adopted new oversight measures, including Corporate Governance Guidelines and Majority Voting Policy.\\n- Achieved 100% Code of Ethics compliance, reinforcing a culture of integrity.\\n- Continued strong Board oversight on risk management, cybersecurity, and sustainability.\\nPhilip Williams, CEO and Director of IsoEnergy, commented, \"This inaugural Report underscores our commitment to responsible resource development and highlights the concrete steps we have taken to reduce our environmental footprint, invest in local communities and businesses, and strengthen partnerships with Indigenous Nations. As we continue advancing our projects, sustainability remains central to how we plan to operate and grow. Looking ahead to 2025, our focus is on formalizing IsoEnergy\\'s sustainability approach by developing a robust ESG framework to guide, manage, and track performance on material ESG issues.\"\\nThe Report sets a strong foundation for IsoEnergy\\'s sustainability journey and outlines the Company\\'s plan to formalize its ESG strategy, including plans to conduct a materiality assessment and develop a Company-wide sustainability policy in 2025.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fc6a275",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'member_descriptor' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\IPython\\core\\formatters.py:770\u001b[39m, in \u001b[36mPlainTextFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    763\u001b[39m stream = StringIO()\n\u001b[32m    764\u001b[39m printer = pretty.RepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m.verbose,\n\u001b[32m    765\u001b[39m     \u001b[38;5;28mself\u001b[39m.max_width, \u001b[38;5;28mself\u001b[39m.newline,\n\u001b[32m    766\u001b[39m     max_seq_length=\u001b[38;5;28mself\u001b[39m.max_seq_length,\n\u001b[32m    767\u001b[39m     singleton_pprinters=\u001b[38;5;28mself\u001b[39m.singleton_printers,\n\u001b[32m    768\u001b[39m     type_pprinters=\u001b[38;5;28mself\u001b[39m.type_printers,\n\u001b[32m    769\u001b[39m     deferred_pprinters=\u001b[38;5;28mself\u001b[39m.deferred_printers)\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m \u001b[43mprinter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m printer.flush()\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\IPython\\lib\\pretty.py:413\u001b[39m, in \u001b[36mRepresentationPrinter.pretty\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    401\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    402\u001b[39m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    403\u001b[39m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    409\u001b[39m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m__repr__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    410\u001b[39m                 ):\n\u001b[32m    411\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m _repr_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    415\u001b[39m     \u001b[38;5;28mself\u001b[39m.end_group()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\IPython\\lib\\pretty.py:600\u001b[39m, in \u001b[36m_default_pprint\u001b[39m\u001b[34m(obj, p, cycle)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    599\u001b[39m p.begin_group(\u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mklass\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m p.text(\u001b[33m'\u001b[39m\u001b[33m at 0x\u001b[39m\u001b[38;5;132;01m%x\u001b[39;00m\u001b[33m'\u001b[39m % \u001b[38;5;28mid\u001b[39m(obj))\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cycle:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\IPython\\lib\\pretty.py:386\u001b[39m, in \u001b[36mRepresentationPrinter.pretty\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _get_mro(obj_class):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_pprinters:\n\u001b[32m    385\u001b[39m         \u001b[38;5;66;03m# printer registered in self.type_pprinters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtype_pprinters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    388\u001b[39m         \u001b[38;5;66;03m# deferred printer\u001b[39;00m\n\u001b[32m    389\u001b[39m         printer = \u001b[38;5;28mself\u001b[39m._in_deferred_types(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\IPython\\lib\\pretty.py:780\u001b[39m, in \u001b[36m_type_pprint\u001b[39m\u001b[34m(obj, p, cycle)\u001b[39m\n\u001b[32m    778\u001b[39m     p.text(name)\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m     p.text(\u001b[43mmod\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m + name)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'member_descriptor' and 'str'"
     ]
    }
   ],
   "source": [
    "nlp(text).sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2e381fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Reclaimed all active exploration sites, and enhanced waste management practices with the goal of reducing material waste.\\n- Improved water efficiency and reduced emissions at Tony M Mine through targeted infrastructure upgrades.\\n- Partnered with communities, establishing investments in people.\\n- Maintained strong Indigenous representation in the workforce with 63% at Matoush and 36% at Larocque East.\\n- Supported community well-being initiatives, including the Northlands College Scholarship Foundation and JZ Memorial Fund.\\n- Implemented Company-wide policies on Health and Safety, Respectful Workplace, and Diversity.\\n- Standardized incident investigation reporting across all exploration projects.\\n- Strengthened governance with new policies for sustainable growth.\\n- Adopted new oversight measures, including Corporate Governance Guidelines and Majority Voting Policy.',\n",
       " '- Achieved 100% Code of Ethics compliance, reinforcing a culture of integrity.\\n- Continued strong Board oversight on risk management, cybersecurity, and sustainability.',\n",
       " 'Philip Williams, CEO and Director of IsoEnergy, commented, \"This inaugural Report underscores our commitment to responsible resource development and highlights the concrete steps we have taken to reduce our environmental footprint, invest in local communities and businesses, and strengthen partnerships with Indigenous Nations.',\n",
       " \"Looking ahead to 2025, our focus is on formalizing IsoEnergy's sustainability approach by developing a robust ESG framework to guide, manage, and track performance on material ESG issues.\",\n",
       " \"The Report sets a strong foundation for IsoEnergy's sustainability journey and outlines the Company's plan to formalize its ESG strategy, including plans to conduct a materiality assessment and develop a Company-wide sustainability policy in 2025.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f9680",
   "metadata": {},
   "source": [
    "spacy has failed miserably..\n",
    "\n",
    "trying out specialized ESGBert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62e00d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'RobertaForSequenceClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DiffLlamaForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GemmaForTokenClassification', 'Gemma2ForTokenClassification', 'GlmForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'HeliumForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LlamaForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MistralForTokenClassification', 'MixtralForTokenClassification', 'MobileBertForTokenClassification', 'ModernBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'MT5ForTokenClassification', 'NemotronForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'PersimmonForTokenClassification', 'PhiForTokenClassification', 'Phi3ForTokenClassification', 'QDQBertForTokenClassification', 'Qwen2ForTokenClassification', 'Qwen2MoeForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'StableLmForTokenClassification', 'Starcoder2ForTokenClassification', 'T5ForTokenClassification', 'UMT5ForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "### MAKE SURE TO INSTALL THIS LIB: !pip install transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline # for using the models\n",
    "\n",
    "### Load the models (takes ca. 1 min)\n",
    "# Environmental model.\n",
    "name = \"ESGBERT/EnvironmentalBERT-environmental\" # path to download from HuggingFace\n",
    "# In simple words, the tokenizer prepares the text for the model and the model classifies the text-\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "# The pipeline combines tokenizer and model to one process.\n",
    "pipe_env = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Also load the social and governance model.\n",
    "# Social model.\n",
    "name = \"ESGBERT/SocialBERT-social\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "pipe_soc = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Governance model.\n",
    "name = \"ESGBERT/GovernanceBERT-governance\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "pipe_gov = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22cd4ef5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m results = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m hybrid_chunks:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     env = \u001b[43mpipe_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     results.append(env)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:250\u001b[39m, in \u001b[36mTokenClassificationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[32m    248\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33moffset_mapping\u001b[39m\u001b[33m\"\u001b[39m] = offset_mapping\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1360\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:327\u001b[39m, in \u001b[36mTokenClassificationPipeline.postprocess\u001b[39m\u001b[34m(self, all_outputs, aggregation_strategy, ignore_labels)\u001b[39m\n\u001b[32m    322\u001b[39m     offset_mapping = offset_mapping.numpy() \u001b[38;5;28;01mif\u001b[39;00m offset_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    324\u001b[39m pre_entities = \u001b[38;5;28mself\u001b[39m.gather_pre_entities(\n\u001b[32m    325\u001b[39m     sentence, input_ids, scores, offset_mapping, special_tokens_mask, aggregation_strategy\n\u001b[32m    326\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m grouped_entities = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_entities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Filter anything that is in self.ignore_labels\u001b[39;00m\n\u001b[32m    329\u001b[39m entities = [\n\u001b[32m    330\u001b[39m     entity\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m grouped_entities\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m entity.get(\u001b[33m\"\u001b[39m\u001b[33mentity\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_labels\n\u001b[32m    333\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m entity.get(\u001b[33m\"\u001b[39m\u001b[33mentity_group\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_labels\n\u001b[32m    334\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\venv\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:428\u001b[39m, in \u001b[36mTokenClassificationPipeline.aggregate\u001b[39m\u001b[34m(self, pre_entities, aggregation_strategy)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pre_entity \u001b[38;5;129;01min\u001b[39;00m pre_entities:\n\u001b[32m    427\u001b[39m     entity_idx = pre_entity[\u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m].argmax()\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     score = \u001b[43mpre_entity\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mentity_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    429\u001b[39m     entity = {\n\u001b[32m    430\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mentity\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model.config.id2label[entity_idx],\n\u001b[32m    431\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: score,\n\u001b[32m   (...)\u001b[39m\u001b[32m    435\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m: pre_entity[\u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    436\u001b[39m     }\n\u001b[32m    437\u001b[39m     entities.append(entity)\n",
      "\u001b[31mIndexError\u001b[39m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for chunk in hybrid_chunks:\n",
    "\n",
    "    env = pipe_env(chunk.text)\n",
    "\n",
    "    results.append(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f75820a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Forw ard-looking statements are necessarily based upon a number of assumptions that, w hile considered reasonable by management at the time, are inherently subject to business, market and economic risks, uncertainties and contingencies that may cause actual results, performance or achievements to be materially different from those expressed or implied by forw ard-looking statements. Such assumptions include, but are not limited to, assumptions that the results of planned ESG activities are as anticipated; the price of uranium; that general business and economic conditions w ill not change in a materially adverse manner; that financing w ill be available if and w hen needed and on reasonable terms; and that third party contractors, equipment and supplies and governmental and other approvals required to conduct the Company's planned activities w ill be available on reasonable terms and in a timely manner. Although IsoEnergy has attempted to identify important factors that could cause actual results to differ materially from those contained in forw ard-looking statements, there may be other factors that cause results not to be as anticipated, estimated or intended. There can be no assurance that such statements w ill prove to be accurate, as actual results and future events could differ materially from those anticipated in such statements. Accordingly, readers should not place undue reliance on forw ard-looking statements.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_chunks[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c459bb",
   "metadata": {},
   "source": [
    "\"Forw ard-looking statements are necessarily based upon a number of assumptions that, w hile considered reasonable by management at the time, are inherently subject to business, market and economic risks, uncertainties and contingencies that may cause actual results, performance or achievements to be materially different from those expressed or implied by forw ard-looking statements. Such assumptions include, but are not limited to, assumptions that the results of planned ESG activities are as anticipated; the price of uranium; that general business and economic conditions w ill not change in a materially adverse manner; that financing w ill be available if and w hen needed and on reasonable terms; and that third party contractors, equipment and supplies and governmental and other approvals required to conduct the Company's planned activities w ill be available on reasonable terms and in a timely manner. Although IsoEnergy has attempted to identify important factors that could cause actual results to differ materially from those contained in forw ard-looking statements, there may be other factors that cause results not to be as anticipated, estimated or intended. There can be no assurance that such statements w ill prove to be accurate, as actual results and future events could differ materially from those anticipated in such statements. Accordingly, readers should not place undue reliance on forw ard-looking statements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa474958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'environmental', 'score': 0.9602301716804504}],\n",
       " [{'label': 'environmental', 'score': 0.9922873973846436}],\n",
       " [{'label': 'none', 'score': 0.7065734267234802}],\n",
       " [{'label': 'none', 'score': 0.9613661766052246}],\n",
       " [{'label': 'environmental', 'score': 0.9746955633163452}],\n",
       " [{'label': 'none', 'score': 0.6120143532752991}]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
