{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Data Processing for RAG with Data Prep Kit (Python)</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2, 3 and 4 in RAG pipeline.\n",
    "\n",
    "Here are the processing steps:\n",
    "\n",
    "- **pdf2parquet** : Extract text (in markdown format) from PDF and store them as parquet files\n",
    "- **Exact Dedup**: Documents with exact content are filtered out\n",
    "- **Chunk documents**: Split the PDFs into 'meaningful sections' (paragraphs, sentences ..etc)\n",
    "- **Text encoder**: Convert chunks into vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8902eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup path to utils folder\n",
    "import sys\n",
    "sys.path.append('../utils')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb3bbc",
   "metadata": {},
   "source": [
    "## Step-2:  Data\n",
    "\n",
    "We will use white papers  about LLMs.  \n",
    "\n",
    "- [Granite Code Models](https://arxiv.org/abs/2405.04324)\n",
    "- [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "You can of course substite your own data below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe7c0c",
   "metadata": {},
   "source": [
    "### 2.1 - Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8739b7a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_file\n\u001b[0;32m      5\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(MY_CONFIG\u001b[38;5;241m.\u001b[39mINPUT_DATA_DIR, ignore_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m shutil\u001b[38;5;241m.\u001b[39mos\u001b[38;5;241m.\u001b[39mmakedirs(MY_CONFIG\u001b[38;5;241m.\u001b[39mINPUT_DATA_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\krish\\Work\\projects\\esg-disclosure-classifier\\src\\dpk_trial\\file_utils.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhumanfriendly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_size\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unquote\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "from file_utils import download_file\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.INPUT_DATA_DIR, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.INPUT_DATA_DIR, exist_ok=True)\n",
    "print (\"✅ Cleared input directory\")\n",
    " \n",
    "download_file (url = 'https://arxiv.org/pdf/1706.03762', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'attention.pdf' ))\n",
    "download_file (url = 'https://arxiv.org/pdf/2405.04324', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'granite.pdf' ))\n",
    "download_file (url = 'https://arxiv.org/pdf/2405.04324', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'granite2.pdf' )) # duplicate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### 2.2 - Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
    "    raise Exception (f\"❌ Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "output_parquet_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '01_parquet_out')\n",
    "output_exact_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '02_dedupe_out')\n",
    "output_chunk_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '03_chunk_out')\n",
    "output_embeddings_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '04_embeddings_out')\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print (\"✅ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "## Step-3: pdf2parquet -  Convert data from PDF to Parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### 3.1 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b101999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃🏼 STAGE-1: Processing input='input' --> output='output/01_parquet_out'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:11:18 INFO - pdf2parquet parameters are : {'batch_size': -1, 'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.MARKDOWN: 'text/markdown'>, 'do_table_structure': True, 'do_ocr': True, 'ocr_engine': <pdf2parquet_ocr_engine.EASYOCR: 'easyocr'>, 'bitmap_area_threshold': 0.05, 'pdf_backend': <pdf2parquet_pdf_backend.DLPARSE_V2: 'dlparse_v2'>, 'double_precision': 8}\n",
      "11:11:18 INFO - pipeline id pipeline_id\n",
      "11:11:18 INFO - code location None\n",
      "11:11:18 INFO - data factory data_ is using local data access: input_folder - input output_folder - output/01_parquet_out\n",
      "11:11:18 INFO - data factory data_ max_files -1, n_sample -1\n",
      "11:11:18 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "11:11:18 INFO - orchestrator pdf2parquet started at 2025-03-14 11:11:18\n",
      "11:11:18 INFO - Number of files is 3, source profile {'max_file_size': 2.112621307373047, 'min_file_size': 1.2146415710449219, 'total_file_size': 4.541904449462891}\n",
      "11:11:18 INFO - Initializing models\n",
      "Could not load the custom kernel for multi-scale deformable attention: Error building extension 'MultiScaleDeformableAttention': [1/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -ccbin /home/sujee/apps/anaconda3/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/TH -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/THC -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mms_deform_attn_cuda.cuda.o \n",
      "/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -ccbin /home/sujee/apps/anaconda3/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/TH -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/THC -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
      "/bin/sh: 1: /usr/bin/nvcc: not found\n",
      "[2/3] /home/sujee/apps/anaconda3/bin/x86_64-conda-linux-gnu-c++ -MMD -MF ms_deform_attn_cpu.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/TH -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/THC -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -DWITH_CUDA=1 -c /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cpu/ms_deform_attn_cpu.cpp -o ms_deform_attn_cpu.o \n",
      "\u001b[31mFAILED: \u001b[0mms_deform_attn_cpu.o \n",
      "/home/sujee/apps/anaconda3/bin/x86_64-conda-linux-gnu-c++ -MMD -MF ms_deform_attn_cpu.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/TH -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/THC -isystem /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -DWITH_CUDA=1 -c /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cpu/ms_deform_attn_cpu.cpp -o ms_deform_attn_cpu.o \n",
      "In file included from /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/ATen/cuda/CUDAContext.h:3,\n",
      "                 from /home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cpu/ms_deform_attn_cpu.cpp:14:\n",
      "/home/sujee/apps/anaconda3/envs/dpk-4-rag-pdf-r1.1.0-py3.11/lib/python3.11/site-packages/torch/include/ATen/cuda/CUDAContextLight.h:6:10: fatal error: cuda_runtime_api.h: No such file or directory\n",
      "    6 | #include <cuda_runtime_api.h>\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~\n",
      "compilation terminated.\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/sujee/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/sujee/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/sujee/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/sujee/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/sujee/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "11:11:35 INFO - Completed 1 files (33.33%) in 0.214 min\n",
      "11:11:54 INFO - Completed 2 files (66.67%) in 0.531 min\n",
      "11:12:13 INFO - Completed 3 files (100.0%) in 0.845 min\n",
      "11:12:13 INFO - Done processing 3 files, waiting for flush() completion.\n",
      "11:12:13 INFO - done flushing in 0.0 sec\n",
      "11:12:13 INFO - Completed execution in 0.911 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage:1 completed successfully\n",
      "CPU times: user 1min 2s, sys: 1.84 s, total: 1min 4s\n",
      "Wall time: 58.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from dpk_pdf2parquet.transform_python import Pdf2Parquet\n",
    "from dpk_pdf2parquet.transform import pdf2parquet_contents_types\n",
    "\n",
    "STAGE = 1 \n",
    "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{MY_CONFIG.INPUT_DATA_DIR}' --> output='{output_parquet_dir}'\\n\", flush=True)\n",
    "\n",
    "result = Pdf2Parquet(input_folder= MY_CONFIG.INPUT_DATA_DIR,\n",
    "                    output_folder= output_parquet_dir,\n",
    "                    data_files_to_use=['.pdf'],\n",
    "                    pdf2parquet_contents_type=pdf2parquet_contents_types.MARKDOWN,   # markdown\n",
    "                    #    pdf2parquet_contents_type=pdf2parquet_contents_types.JSON   # JSON\n",
    "                    ).transform()\n",
    "\n",
    "if result == 0:\n",
    "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (f\"❌ Stage:{STAGE}  failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790e0",
   "metadata": {},
   "source": [
    "### 3.2 -  Inspect Generated output\n",
    "\n",
    "Here we should see one entry per input file processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe59563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 3 parquet files with 3 total rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_hash</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>436</td>\n",
       "      <td>cebabb79-b5ba-4955-91bc-293c9f0052f1</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...</td>\n",
       "      <td>45855</td>\n",
       "      <td>2025-03-14T11:11:35.158814</td>\n",
       "      <td>12.844160</td>\n",
       "      <td>attention.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>granite2.pdf</td>\n",
       "      <td>## Granite Code Models: A Family of Open Found...</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>324</td>\n",
       "      <td>540c13b9-50c6-4079-a214-4fb720d391ee</td>\n",
       "      <td>3127757990743433032</td>\n",
       "      <td>pdf</td>\n",
       "      <td>bf9642b9f719c6c7db1ad66a2992c96a755339ba9e793e...</td>\n",
       "      <td>136086</td>\n",
       "      <td>2025-03-14T11:12:13.016518</td>\n",
       "      <td>18.821193</td>\n",
       "      <td>granite2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>## Granite Code Models: A Family of Open Found...</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>324</td>\n",
       "      <td>78762d84-35f0-4bcb-8785-622a730c37ed</td>\n",
       "      <td>3127757990743433032</td>\n",
       "      <td>pdf</td>\n",
       "      <td>bf9642b9f719c6c7db1ad66a2992c96a755339ba9e793e...</td>\n",
       "      <td>136086</td>\n",
       "      <td>2025-03-14T11:11:54.146730</td>\n",
       "      <td>18.945178</td>\n",
       "      <td>granite.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                           contents  \\\n",
       "0  attention.pdf  Provided proper attribution is provided, Googl...   \n",
       "1   granite2.pdf  ## Granite Code Models: A Family of Open Found...   \n",
       "2    granite.pdf  ## Granite Code Models: A Family of Open Found...   \n",
       "\n",
       "   num_pages  num_tables  num_doc_elements  \\\n",
       "0         15           6               436   \n",
       "1         28          21               324   \n",
       "2         28          21               324   \n",
       "\n",
       "                            document_id        document_hash  ext  \\\n",
       "0  cebabb79-b5ba-4955-91bc-293c9f0052f1  2949302674760005271  pdf   \n",
       "1  540c13b9-50c6-4079-a214-4fb720d391ee  3127757990743433032  pdf   \n",
       "2  78762d84-35f0-4bcb-8785-622a730c37ed  3127757990743433032  pdf   \n",
       "\n",
       "                                                hash    size  \\\n",
       "0  5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...   45855   \n",
       "1  bf9642b9f719c6c7db1ad66a2992c96a755339ba9e793e...  136086   \n",
       "2  bf9642b9f719c6c7db1ad66a2992c96a755339ba9e793e...  136086   \n",
       "\n",
       "                date_acquired  pdf_convert_time source_filename  \n",
       "0  2025-03-14T11:11:35.158814         12.844160   attention.pdf  \n",
       "1  2025-03-14T11:12:13.016518         18.821193    granite2.pdf  \n",
       "2  2025-03-14T11:11:54.146730         18.945178     granite.pdf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from file_utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_parquet_dir)\n",
    "\n",
    "# print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(5)\n",
    "\n",
    "## To display certain columns\n",
    "#parquet_df[['column1', 'column2', 'column3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f900753",
   "metadata": {},
   "source": [
    "## Step-4: Eliminate Duplicate Documents\n",
    "\n",
    "We have 2 duplicate documnets here : `granite.pdf` and `granite2.pdf`.\n",
    "\n",
    "Note how the `hash` for these documents are same.\n",
    "\n",
    "We are going to perform **de-dupe**\n",
    "\n",
    "On the content of each document, a SHA256 hash is computed, followed by de-duplication of record having identical hashes.\n",
    "\n",
    "[Dedupe transform documentation](https://github.com/data-prep-kit/data-prep-kit/blob/dev/transforms/universal/ededup/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef93831",
   "metadata": {},
   "source": [
    "### 4.1 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1901b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃🏼 STAGE-2: Processing input='output/01_parquet_out' --> output='output/02_dedupe_out'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:12:13 INFO - exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'document_id', 'use_snapshot': False, 'snapshot_directory': None}\n",
      "11:12:13 INFO - pipeline id pipeline_id\n",
      "11:12:13 INFO - code location None\n",
      "11:12:13 INFO - data factory data_ is using local data access: input_folder - output/01_parquet_out output_folder - output/02_dedupe_out\n",
      "11:12:13 INFO - data factory data_ max_files -1, n_sample -1\n",
      "11:12:13 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "11:12:13 INFO - orchestrator ededup started at 2025-03-14 11:12:13\n",
      "11:12:13 INFO - Number of files is 3, source profile {'max_file_size': 0.0434112548828125, 'min_file_size': 0.020546913146972656, 'total_file_size': 0.10735607147216797}\n",
      "11:12:13 INFO - Starting from the beginning\n",
      "11:12:13 INFO - Completed 1 files (33.33%) in 0.0 min\n",
      "11:12:13 INFO - Completed 2 files (66.67%) in 0.0 min\n",
      "11:12:13 INFO - Completed 3 files (100.0%) in 0.0 min\n",
      "11:12:13 INFO - Done processing 3 files, waiting for flush() completion.\n",
      "11:12:13 INFO - done flushing in 0.0 sec\n",
      "11:12:13 INFO - Completed execution in 0.001 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage:2 completed successfully\n",
      "CPU times: user 37.7 ms, sys: 3.92 ms, total: 41.6 ms\n",
      "Wall time: 36.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from dpk_ededup.transform_python import Ededup\n",
    "\n",
    "STAGE = 2\n",
    "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{output_parquet_dir}' --> output='{output_exact_dedupe_dir}'\\n\", flush=True)\n",
    "\n",
    "result = Ededup(input_folder=output_parquet_dir,\n",
    "    output_folder=output_exact_dedupe_dir,\n",
    "    ededup_doc_column=\"contents\",\n",
    "    ededup_doc_id_column=\"document_id\"\n",
    "    ).transform()\n",
    "\n",
    "if result == 0:\n",
    "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (f\"❌ Stage:{STAGE}  failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a59d2",
   "metadata": {},
   "source": [
    "### 4.2 - Inspect Generated output\n",
    "\n",
    "We would see 2 documents: `attention.pdf`  and `granite.pdf`.  The duplicate `granite.pdf` has been filtered out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0691f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 3 parquet files with 3 total rows\n",
      "Successfully read 2 parquet files with 2 total rows\n",
      "Input files before exact dedupe : 3\n",
      "Output files after exact dedupe : 2\n",
      "Duplicate files removed :   1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_hash</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>436</td>\n",
       "      <td>cebabb79-b5ba-4955-91bc-293c9f0052f1</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...</td>\n",
       "      <td>45855</td>\n",
       "      <td>2025-03-14T11:11:35.158814</td>\n",
       "      <td>12.844160</td>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>## Granite Code Models: A Family of Open Found...</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>324</td>\n",
       "      <td>78762d84-35f0-4bcb-8785-622a730c37ed</td>\n",
       "      <td>3127757990743433032</td>\n",
       "      <td>pdf</td>\n",
       "      <td>bf9642b9f719c6c7db1ad66a2992c96a755339ba9e793e...</td>\n",
       "      <td>136086</td>\n",
       "      <td>2025-03-14T11:11:54.146730</td>\n",
       "      <td>18.945178</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                           contents  \\\n",
       "0  attention.pdf  Provided proper attribution is provided, Googl...   \n",
       "1    granite.pdf  ## Granite Code Models: A Family of Open Found...   \n",
       "\n",
       "   num_pages  num_tables  num_doc_elements  \\\n",
       "0         15           6               436   \n",
       "1         28          21               324   \n",
       "\n",
       "                            document_id        document_hash  ext  \\\n",
       "0  cebabb79-b5ba-4955-91bc-293c9f0052f1  2949302674760005271  pdf   \n",
       "1  78762d84-35f0-4bcb-8785-622a730c37ed  3127757990743433032  pdf   \n",
       "\n",
       "                                                hash    size  \\\n",
       "0  5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...   45855   \n",
       "1  bf9642b9f719c6c7db1ad66a2992c96a755339ba9e793e...  136086   \n",
       "\n",
       "                date_acquired  pdf_convert_time source_filename removed  \n",
       "0  2025-03-14T11:11:35.158814         12.844160   attention.pdf      []  \n",
       "1  2025-03-14T11:11:54.146730         18.945178     granite.pdf      []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from file_utils import read_parquet_files_as_df\n",
    "\n",
    "input_df = read_parquet_files_as_df(output_parquet_dir)\n",
    "output_df = read_parquet_files_as_df(output_exact_dedupe_dir)\n",
    "\n",
    "# print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "# print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (f\"Input files before exact dedupe : {input_df.shape[0]:,}\")\n",
    "print (f\"Output files after exact dedupe : {output_df.shape[0]:,}\")\n",
    "print (\"Duplicate files removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72274586",
   "metadata": {},
   "source": [
    "##  Step-5: Doc chunks\n",
    "\n",
    "Split the documents in chunks.\n",
    "\n",
    "[Chunking transform documentation](https://github.com/data-prep-kit/data-prep-kit/blob/dev/transforms/language/doc_chunk/README.md)\n",
    "\n",
    "**Experiment with chunking size to find the setting that works best for your documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f2cd1",
   "metadata": {},
   "source": [
    "### 5.1 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cfbf532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃🏼 STAGE-3: Processing input='output/02_dedupe_out' --> output='output/03_chunk_out'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:12:13 INFO - doc_chunk parameters are : {'chunking_type': 'li_markdown', 'content_column_name': 'contents', 'doc_id_column_name': 'document_id', 'output_chunk_column_name': 'contents', 'output_source_doc_id_column_name': 'source_document_id', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox', 'chunk_size_tokens': 128, 'chunk_overlap_tokens': 30, 'dl_min_chunk_len': None}\n",
      "11:12:13 INFO - pipeline id pipeline_id\n",
      "11:12:13 INFO - code location None\n",
      "11:12:13 INFO - data factory data_ is using local data access: input_folder - output/02_dedupe_out output_folder - output/03_chunk_out\n",
      "11:12:13 INFO - data factory data_ max_files -1, n_sample -1\n",
      "11:12:13 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "11:12:13 INFO - orchestrator doc_chunk started at 2025-03-14 11:12:13\n",
      "11:12:13 INFO - Number of files is 3, source profile {'max_file_size': 0.043753623962402344, 'min_file_size': 0.0028095245361328125, 'total_file_size': 0.06746578216552734}\n",
      "11:12:13 INFO - Completed 1 files (33.33%) in 0.0 min\n",
      "11:12:13 INFO - Completed 2 files (66.67%) in 0.0 min\n",
      "11:12:13 WARNING - table is empty, skipping processing\n",
      "11:12:13 INFO - Completed 3 files (100.0%) in 0.0 min\n",
      "11:12:13 INFO - Done processing 3 files, waiting for flush() completion.\n",
      "11:12:13 INFO - done flushing in 0.0 sec\n",
      "11:12:13 INFO - Completed execution in 0.001 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage:3 completed successfully\n",
      "CPU times: user 656 ms, sys: 86.2 ms, total: 742 ms\n",
      "Wall time: 736 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from dpk_doc_chunk.transform_python import DocChunk\n",
    "\n",
    "STAGE = 3\n",
    "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{output_exact_dedupe_dir}' --> output='{output_chunk_dir}'\\n\", flush=True)\n",
    "\n",
    "result = DocChunk(input_folder=output_exact_dedupe_dir,\n",
    "        output_folder=output_chunk_dir,\n",
    "        doc_chunk_chunking_type= \"li_markdown\",\n",
    "        # doc_chunk_chunking_type= \"dl_json\",\n",
    "        doc_chunk_chunk_size_tokens = 128,  # default 128\n",
    "        doc_chunk_chunk_overlap_tokens=30   # default 30\n",
    "        ).transform()\n",
    "\n",
    "if result == 0:\n",
    "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (f\"❌ Stage:{STAGE}  failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213afdf6",
   "metadata": {},
   "source": [
    "### 5.2 - Inspect Generated output\n",
    "\n",
    "We would see documents are split into many chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8138d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 2 parquet files with 2 total rows\n",
      "Successfully read 2 parquet files with 61 total rows\n",
      "Files processed : 2\n",
      "Chunks created : 61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_hash</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>removed</th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>436</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...</td>\n",
       "      <td>45855</td>\n",
       "      <td>2025-03-14T11:11:35.158814</td>\n",
       "      <td>12.844160</td>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>cebabb79-b5ba-4955-91bc-293c9f0052f1</td>\n",
       "      <td>## 3.5 Positional Encoding\\n\\nSince our model ...</td>\n",
       "      <td>cb11d177cf91a1d4e6dfeefc101bb2b597aaf6254efdbc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>436</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...</td>\n",
       "      <td>45855</td>\n",
       "      <td>2025-03-14T11:11:35.158814</td>\n",
       "      <td>12.844160</td>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>cebabb79-b5ba-4955-91bc-293c9f0052f1</td>\n",
       "      <td>## 3.2.1 Scaled Dot-Product Attention\\n\\nWe ca...</td>\n",
       "      <td>372f585509e5743de4e6603483cad7dfa3bd5bbdfa131c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>324</td>\n",
       "      <td>3127757990743433032</td>\n",
       "      <td>pdf</td>\n",
       "      <td>bf9642b9f719c6c7db1ad66a2992c96a755339ba9e793e...</td>\n",
       "      <td>136086</td>\n",
       "      <td>2025-03-14T11:11:54.146730</td>\n",
       "      <td>18.945178</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>78762d84-35f0-4bcb-8785-622a730c37ed</td>\n",
       "      <td>## 6.1.1 HumanEvalSynthesize: Multilingual Cod...</td>\n",
       "      <td>88dab0e6c051bffc8c4bcf9c070da2d1fe75c0a214b41b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  num_pages  num_tables  num_doc_elements  \\\n",
       "14  attention.pdf         15           6               436   \n",
       "9   attention.pdf         15           6               436   \n",
       "45    granite.pdf         28          21               324   \n",
       "\n",
       "          document_hash  ext  \\\n",
       "14  2949302674760005271  pdf   \n",
       "9   2949302674760005271  pdf   \n",
       "45  3127757990743433032  pdf   \n",
       "\n",
       "                                                 hash    size  \\\n",
       "14  5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...   45855   \n",
       "9   5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...   45855   \n",
       "45  bf9642b9f719c6c7db1ad66a2992c96a755339ba9e793e...  136086   \n",
       "\n",
       "                 date_acquired  pdf_convert_time source_filename removed  \\\n",
       "14  2025-03-14T11:11:35.158814         12.844160   attention.pdf      []   \n",
       "9   2025-03-14T11:11:35.158814         12.844160   attention.pdf      []   \n",
       "45  2025-03-14T11:11:54.146730         18.945178     granite.pdf      []   \n",
       "\n",
       "                      source_document_id  \\\n",
       "14  cebabb79-b5ba-4955-91bc-293c9f0052f1   \n",
       "9   cebabb79-b5ba-4955-91bc-293c9f0052f1   \n",
       "45  78762d84-35f0-4bcb-8785-622a730c37ed   \n",
       "\n",
       "                                             contents  \\\n",
       "14  ## 3.5 Positional Encoding\\n\\nSince our model ...   \n",
       "9   ## 3.2.1 Scaled Dot-Product Attention\\n\\nWe ca...   \n",
       "45  ## 6.1.1 HumanEvalSynthesize: Multilingual Cod...   \n",
       "\n",
       "                                          document_id  \n",
       "14  cb11d177cf91a1d4e6dfeefc101bb2b597aaf6254efdbc...  \n",
       "9   372f585509e5743de4e6603483cad7dfa3bd5bbdfa131c...  \n",
       "45  88dab0e6c051bffc8c4bcf9c070da2d1fe75c0a214b41b...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from file_utils import read_parquet_files_as_df\n",
    "\n",
    "input_df = read_parquet_files_as_df(output_exact_dedupe_dir)  ## for debug purposes\n",
    "output_df = read_parquet_files_as_df(output_chunk_dir)\n",
    "\n",
    "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
    "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
    "\n",
    "# print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "# print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## Step-6:   Calculate Embeddings for Chunks\n",
    "\n",
    "we will calculate embeddings for each chunk using an open source embedding model\n",
    "\n",
    "[Embeddings / Text Encoder documentation](https://github.com/data-prep-kit/data-prep-kit/blob/dev/transforms/language/text_encoder/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9112479",
   "metadata": {},
   "source": [
    "### 6.1 - Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e8b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃🏼 STAGE-4: Processing input='output/03_chunk_out' --> output='output/04_embeddings_out'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:12:14 INFO - text_encoder parameters are : {'content_column_name': 'contents', 'output_embeddings_column_name': 'embeddings', 'model_name': 'ibm-granite/granite-embedding-30m-english'}\n",
      "11:12:14 INFO - pipeline id pipeline_id\n",
      "11:12:14 INFO - code location None\n",
      "11:12:14 INFO - data factory data_ is using local data access: input_folder - output/03_chunk_out output_folder - output/04_embeddings_out\n",
      "11:12:14 INFO - data factory data_ max_files -1, n_sample -1\n",
      "11:12:14 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "11:12:14 INFO - orchestrator text_encoder started at 2025-03-14 11:12:14\n",
      "11:12:14 INFO - Number of files is 2, source profile {'max_file_size': 0.04574298858642578, 'min_file_size': 0.028684616088867188, 'total_file_size': 0.07442760467529297}\n",
      "11:12:16 INFO - Completed 1 files (50.0%) in 0.003 min\n",
      "11:12:17 INFO - Completed 2 files (100.0%) in 0.006 min\n",
      "11:12:17 INFO - Done processing 2 files, waiting for flush() completion.\n",
      "11:12:17 INFO - done flushing in 0.0 sec\n",
      "11:12:17 INFO - Completed execution in 0.05 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage:4 completed successfully\n",
      "CPU times: user 1.03 s, sys: 164 ms, total: 1.19 s\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from dpk_text_encoder.transform_python import TextEncoder\n",
    "\n",
    "STAGE  = 4\n",
    "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{output_chunk_dir}' --> output='{output_embeddings_dir}'\\n\", flush=True)\n",
    "\n",
    "\n",
    "result = TextEncoder(input_folder= output_chunk_dir, \n",
    "               output_folder= output_embeddings_dir, \n",
    "               text_encoder_model_name = MY_CONFIG.EMBEDDING_MODEL\n",
    "               ).transform()\n",
    "if result == 0:\n",
    "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (f\"❌ Stage:{STAGE}  failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734852c",
   "metadata": {},
   "source": [
    "### 6.2 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1c1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 2 parquet files with 61 total rows\n",
      "Successfully read 2 parquet files with 61 total rows\n",
      "Input data dimensions (rows x columns)=  (61, 15)\n",
      "Output data dimensions (rows x columns)=  (61, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_hash</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>removed</th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>document_id</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>436</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...</td>\n",
       "      <td>45855</td>\n",
       "      <td>2025-03-14T11:11:35.158814</td>\n",
       "      <td>12.84416</td>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>cebabb79-b5ba-4955-91bc-293c9f0052f1</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>cb3a9de8b5457dde64115f2fcd6e5b4093b228499af4ac...</td>\n",
       "      <td>[-0.054784447, 0.049434207, 0.0080653345, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>436</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...</td>\n",
       "      <td>45855</td>\n",
       "      <td>2025-03-14T11:11:35.158814</td>\n",
       "      <td>12.84416</td>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>cebabb79-b5ba-4955-91bc-293c9f0052f1</td>\n",
       "      <td>## 3.2 Attention\\n\\nAn attention function can ...</td>\n",
       "      <td>bac7e7f7e0121639aa67e546cc42f16c1b1b3a4c5083f8...</td>\n",
       "      <td>[0.021459015, 0.05516718, -0.041755836, 0.0438...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>436</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...</td>\n",
       "      <td>45855</td>\n",
       "      <td>2025-03-14T11:11:35.158814</td>\n",
       "      <td>12.84416</td>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>cebabb79-b5ba-4955-91bc-293c9f0052f1</td>\n",
       "      <td>## 3 Model Architecture\\n\\nMost competitive ne...</td>\n",
       "      <td>b8ed30a0030817a11d373ce52e26bf6edb0a713571efc9...</td>\n",
       "      <td>[-0.018718036, -0.01040597, 0.047386818, 0.010...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  num_pages  num_tables  num_doc_elements  \\\n",
       "0  attention.pdf         15           6               436   \n",
       "7  attention.pdf         15           6               436   \n",
       "5  attention.pdf         15           6               436   \n",
       "\n",
       "         document_hash  ext  \\\n",
       "0  2949302674760005271  pdf   \n",
       "7  2949302674760005271  pdf   \n",
       "5  2949302674760005271  pdf   \n",
       "\n",
       "                                                hash   size  \\\n",
       "0  5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...  45855   \n",
       "7  5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...  45855   \n",
       "5  5bbff6b2beac5f09f368a913932212e8d57953b3e3e86d...  45855   \n",
       "\n",
       "                date_acquired  pdf_convert_time source_filename removed  \\\n",
       "0  2025-03-14T11:11:35.158814          12.84416   attention.pdf      []   \n",
       "7  2025-03-14T11:11:35.158814          12.84416   attention.pdf      []   \n",
       "5  2025-03-14T11:11:35.158814          12.84416   attention.pdf      []   \n",
       "\n",
       "                     source_document_id  \\\n",
       "0  cebabb79-b5ba-4955-91bc-293c9f0052f1   \n",
       "7  cebabb79-b5ba-4955-91bc-293c9f0052f1   \n",
       "5  cebabb79-b5ba-4955-91bc-293c9f0052f1   \n",
       "\n",
       "                                            contents  \\\n",
       "0  Provided proper attribution is provided, Googl...   \n",
       "7  ## 3.2 Attention\\n\\nAn attention function can ...   \n",
       "5  ## 3 Model Architecture\\n\\nMost competitive ne...   \n",
       "\n",
       "                                         document_id  \\\n",
       "0  cb3a9de8b5457dde64115f2fcd6e5b4093b228499af4ac...   \n",
       "7  bac7e7f7e0121639aa67e546cc42f16c1b1b3a4c5083f8...   \n",
       "5  b8ed30a0030817a11d373ce52e26bf6edb0a713571efc9...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.054784447, 0.049434207, 0.0080653345, 0.00...  \n",
       "7  [0.021459015, 0.05516718, -0.041755836, 0.0438...  \n",
       "5  [-0.018718036, -0.01040597, 0.047386818, 0.010...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from file_utils import read_parquet_files_as_df\n",
    "\n",
    "input_df = read_parquet_files_as_df(output_chunk_dir)\n",
    "output_df = read_parquet_files_as_df(output_embeddings_dir)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12630-be6b-4188-a925-77117155617b",
   "metadata": {},
   "source": [
    "## Step-7: Copy output to final output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Copied output from 'output/04_embeddings_out' --> 'output/output_final'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
    "shutil.copytree(src=output_embeddings_dir, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
    "\n",
    "print (f\"✅ Copied output from '{output_embeddings_dir}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
